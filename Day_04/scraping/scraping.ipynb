{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: O’Reilly Books About Data\n",
    "A potential investor in DataSciencester thinks data is just a fad. To prove him wrong, you\n",
    "decide to examine how many data books O’Reilly has published over time. After digging\n",
    "through its website, you find that it has many pages of data books (and videos), reachable\n",
    "through 30-items-at-a-time directory pages with URLs like:\n",
    "```\n",
    "http://shop.oreilly.com/category/browse-subjects/data.do?\n",
    "sortby=publicationDate&page=1\n",
    "```\n",
    "\n",
    "Unless you want to be a jerk (and unless you want your scraper to get banned), whenever\n",
    "you want to scrape data from a website you should first check to see if it has some sort of\n",
    "access policy. Looking at:\n",
    "```\n",
    "http://oreilly.com/terms/\n",
    "```\n",
    "\n",
    "there seems to be nothing prohibiting this project. In order to be good citizens, we should\n",
    "also check for a robots.txt file that tells webcrawlers how to behave. The important lines in\n",
    "```\n",
    "http://shop.oreilly.com/robots.txt are:\n",
    "Crawl-delay: 30\n",
    "Request-rate: 1/30\n",
    "```\n",
    "The first tells us that we should wait 30 seconds between requests, the second that we\n",
    "should request only one page every 30 seconds. So basically they’re two different ways of\n",
    "saying the same thing. (There are other lines that indicate directories not to scrape, but\n",
    "they don’t include our URL, so we’re OK there.)\n",
    "\n",
    "`NOTE`\n",
    "\n",
    "There’s always the possibility that O’Reilly will at some point revamp its website and break all the logic in\n",
    "this section. I will do what I can to prevent that, of course, but I don’t have a ton of influence over there.\n",
    "Although, if every one of you were to convince everyone you know to buy a copy of this book…\n",
    "To figure out how to extract the data, let’s download one of those pages and feed it to\n",
    "Beautiful Soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyppeteer in c:\\users\\talha\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\talha\\anaconda3\\lib\\site-packages (1.5.6)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in c:\\users\\talha\\anaconda3\\lib\\site-packages (from pyppeteer) (1.4.4)\n",
      "Requirement already satisfied: certifi>=2023 in c:\\users\\talha\\anaconda3\\lib\\site-packages (from pyppeteer) (2024.2.2)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\talha\\anaconda3\\lib\\site-packages (from pyppeteer) (6.0.0)\n",
      "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in c:\\users\\talha\\anaconda3\\lib\\site-packages (from pyppeteer) (11.1.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in c:\\users\\talha\\anaconda3\\lib\\site-packages (from pyppeteer) (4.65.0)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in c:\\users\\talha\\anaconda3\\lib\\site-packages (from pyppeteer) (1.26.16)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in c:\\users\\talha\\anaconda3\\lib\\site-packages (from pyppeteer) (10.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\talha\\anaconda3\\lib\\site-packages (from importlib-metadata>=1.4->pyppeteer) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\talha\\anaconda3\\lib\\site-packages (from pyee<12.0.0,>=11.0.0->pyppeteer) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\talha\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyppeteer nest_asyncio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Path to your WebDriver\n",
    "driver_path = './chromedriver.exe' \n",
    "c_service = webdriver.ChromeService(executable_path=driver_path)\n",
    "def fetch_page(url, target):\n",
    "    driver = webdriver.Chrome(service=c_service)\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, target)))\n",
    "    \n",
    "    page_source = driver.page_source\n",
    "    \n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    return page_source\n",
    "\n",
    "url = 'http://shop.oreilly.com/category/browse-subjects/data.do?sortby=publicationDate&page=1'\n",
    "target = '.css-ataarr'\n",
    "\n",
    "page_content = fetch_page(url, target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page_content, 'html5lib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "filtered = soup.find_all('article' , {'class' : 'css-ataarr'})\n",
    "types = filtered[0].find_all('div' , {'data-testid' : \"ContextLabel/ContentLevels\"})\n",
    "print(filtered[0].find('span' , {'data-testid' : 'ContextLabel/ContentLevels'})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Book\n"
     ]
    }
   ],
   "source": [
    "for item in types:\n",
    "    print('---------------------------------------------')\n",
    "    print(item.find('strong').text.split(':')[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
